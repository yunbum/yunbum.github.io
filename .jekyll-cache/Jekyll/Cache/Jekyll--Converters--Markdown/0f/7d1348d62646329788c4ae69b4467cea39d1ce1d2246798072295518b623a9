I"°8<h2 id="background">Background</h2>
<p>GPS ì˜ ì •ë°€ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ RTK mode ì¤‘ Network RTK ê¸°ëŠ¥ ì„¤ì •ì„ ìœ„í•´ í•„ìš”í•œ Ntrip client íˆ´ì„ ì§ì ‘ ê°œë°œí•˜ì—¬ ì‚¬ìš©í•˜ê³ , ë³„ë¡œë„ ë…ë¦½ íˆ´ë¡œ ê°œë°œí•˜ì—¬ ë°°í¬í•©ë‹ˆë‹¤..</p>

<p><img src="./assets/img/posts/20210318/RL_Connect.jpg" alt="RL_Connect" />
<small>[RL_Connect] Netwrok RTK ëª¨ë“œ ì„¤ì •ì„ ìœ„í•œ Ntrip client..</small></p>

<center><img style="float: left;margin-right: 1em;" src="./assets/img/posts/20210318/tm_circle.png" width="310" height="300" /></center>

<p>You can read the blog post for it <a href="./ML-Library-from-scratch.html">here</a>.</p>

<p>I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a> or hardcoded (an exercise I wanted to do since a long time).</p>

<ul><li>Basic Ntirp client.</li><li>FKP, VRS mount ê¸°ì¤€êµ­ ì ‘ì† ê°€ëŠ¥.</li><li>After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training round</li><li>A secondary </li></ul>

<h2 id="designing-the-neural-network">Designing the neural network</h2>

<center><img src="./assets/img/posts/20210318/Neural_Network_Topology.png" width="540" /></center>
<p><br /></p>

<p>The Neural Network c</p>

<p>I started out with two hidden layers of 36 neurons each.</p>

<h2 id="the-many-models">The many modelsâ€¦</h2>
<h3 id="model-1---the-first-try">Model 1 - the first try</h3>

<p>At first the model was trained by playing vs. a â€œperfectâ€ AI, meaning a <a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Game.py#L43">hard coded algorithm</a> that</p>

<p>I came across a <a href="https://github.com/bckenstler/CLR">technique by Brad Kenstler, Carl Thome and Jeremy Jordan</a> called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks.</p>

<center><img src="./assets/img/posts/20210318/lr_formula.jpeg" width="280" /></center>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">true_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">-</span> <span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">c</span><span class="p">.</span><span class="n">DECAY_RATE</span><span class="o">*</span><span class="n">true_epoch</span><span class="p">))</span>
<span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">CLR_ON</span><span class="p">:</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">true_epoch</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">cycle</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span><span class="o">+</span><span class="p">(</span><span class="n">max_lr</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p><br />With these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves6.png" width="540" />
<small>1,000,000 episodes, 7.5 million epochs with batches of 64 moves each<br />
Wins: 52.66% Losses: 36.02% Ties: 11.32%</small></center>

<p>After <strong>24 hours!</strong>, my computer 
<a name="Model3"></a></p>
<h3 id="model-3---new-network-topology">Model 3 - new network topology</h3>

<p>After all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.</p>

<p><strong>Finally tested FKP,VRS</strong> This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.</p>

<p>It is quite interesting :</p>
<ul>
  <li>the rewards policy</li>
  <li>the epsilon greedy strategy</li>
  <li>whether to train vs. a random player or an â€œintelligentâ€ AI.</li>
</ul>

<p>And so far the most effective change</p>

<p><a name="Model4"></a></p>
<h3 id="model-4---implementing-momentum">Model 4 - implementing momentum</h3>

<p>I <a href="https://www.reddit.com/r/MachineLearning/comments/lzvrwp/p_help_with_a_reinforcement_learning_project/">reached out to the reddit community</a> and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm.</p>

<ul>
  <li>Stochastic Gradient Descent with Momentum</li>
  <li>RMSProp: Root Mean Square Plain Momentum</li>
</ul>

<p><a name="optimization"></a><a href="https://the-mvm.github.io/neural-network-optimization-methods/">Click here for a detailed explanation and code of all the implemented optimization algorithms.</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span>
                           <span class="n">units</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span>
                           <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                           <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span>
                        <span class="n">outputs</span><span class="p">,</span>
                        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                        <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
           <span class="n">beta_1</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">GAMMA_OPT</span><span class="p">,</span>
           <span class="n">beta_2</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BETA</span><span class="p">,</span>
           <span class="n">epsilon</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">,</span>
           <span class="n">amsgrad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
                           <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                           <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>
<p>As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.</p>

<p>The training function changed to:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reduce_lr_on_plateau</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s">'loss'</span><span class="p">,</span>
                                         <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                         <span class="n">patience</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">states_to_train</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">targets_to_train</span><span class="p">),</span>
                                 <span class="n">epochs</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPOCHS</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                 <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">reduce_lr_on_plateau</span><span class="p">],</span>
                                 <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didnâ€™t change a thing on the training of the network, so the results kept being the same, <strong>the loss function was still stagnating! My code was not the issue.</strong>
<a name="Model7"></a></p>
<h3 id="model-7---changing-the-training-schedule">Model 7 - changing the training schedule</h3>
<p>Next I tried to change the way the network was training as per <a href="https://www.reddit.com/user/elBarto015">u/elBarto015</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lzzjar/i_created_an_ai_for_super_hexagon_based_on/gqc8ka6?utm_source=share&amp;utm_medium=web2x&amp;context=3">advised me on reddit</a>.</p>

<p>The way I was training initially was:</p>
<ul>
  <li>Games begin being simulated and the outcome recorded in the replay memory</li>
  <li>Once a sufficient ammount of experiences are recorded (at least equal to the batch</li>
</ul>

<center><img src="./assets/img/posts/20210318/ReplayMemoryBefore.png" width="540" /></center>

<p>As of today, <a href="https://medium.com/applied-data-science/how-to-train-ai-agents-to-play-multiplayer-games-using-self-play-deep-reinforcement-learning-247d0b440717">self play</a>, and it looks like a viable option to test and a fun coding challenge.</p>
:ET