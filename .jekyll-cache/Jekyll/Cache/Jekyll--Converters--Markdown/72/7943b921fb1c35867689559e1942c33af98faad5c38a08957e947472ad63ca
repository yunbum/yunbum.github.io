I"l<p>ëª¨ë°”ì¼ ë¡œë´‡ì— ì¥ì°©í•˜ê±°ë‚˜ ì—°ê²°í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì•…ì„¸ì‚¬ë¦¬ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<center><img src="./assets/img/posts/20210228/trailer-1-c.png" width="480px" /></center>
<p>Let me try to explain; I am in the process of immersing myself into the world of Machine Learnin.</p>

<p>Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.</p>

<center><img src="./assets/img/posts/20210228/nnet_flow.gif" /></center>

<p>The library started very narrowly, with just the following functionality:</p>
<ul>
  <li><strong>create</strong> a neural network based on the following parameters:
    <ul>
      <li>number of inputs</li>
      <li>size and number of hidden layers</li>
      <li>number of outputs</li>
      <li>learning rate</li>
    </ul>
  </li>
  <li><strong>forward propagate</strong> or predict the output values when given some inputs</li>
  <li><strong>learn</strong> through back propagation using gradient descent</li>
</ul>

<p>I restricted the model to be sequential, the only activation function I implemented was sigmoid:</p>

<center><img src="./assets/img/posts/20210228/nn_diagram.png" /></center>

<p>With my neural network coded, I tested it with a very basic problem, the famous XOR problem.</p>

<p>XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:</p>

<center><img src="./assets/img/posts/20210228/xor_problem.png" /></center>

<p>As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true.</p>

<p>As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">axs1</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span>
                  <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span>
                  <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">antialiased</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>The end result looks something like this:</p>
<center><img src="./assets/img/posts/20210228/Surface_XOR.jpg" /></center>

<p>Then we reshape the z array as a one dimensional array to use it to color the scatter plot:</p>

<center><img src="./assets/img/posts/20210228/Final_XOR_Plot.jpg" /></center>

<p>So my baby ML library is completed for now, but still I would like to enhance it in several ways:</p>

<ul>
  <li>include multiple activation functions (ReLu, linear, Tanh, etc.)</li>
  <li>allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.)</li>
  <li>have batch and epoch training schedules functionality</li>
  <li>save and load trained model to file</li>
</ul>

<p>I will get to it soonâ€¦</p>
:ET