I"5,<h2 id="background">Background</h2>
<p>GPS 의 정밀도를 높이기 위한 RTK mode 중 Network RTK 기능 설정을 위해 필요한 Ntrip client 툴을 직접 개발하여 사용하고, 별로도 독립 툴로 개발하여 배포합니다..</p>

<p><img src="./assets/img/posts/20210318/RL_Connect.jpg" alt="RL_Connect" />
<small>[RL_Connect] Netwrok RTK 모드 설정을 위한 Ntrip client..</small></p>

<center><img style="float: left;margin-right: 1em;" src="./assets/img/posts/20210318/tm_circle.png" width="310" height="300" /></center>

<p>위도/경도 를 TM 좌표계로 변환한 위치에 Array 지점들을 표시하여 모든 점들을 포함하는 최소원을 그래프에 나타내고 원의 반경을 계산하여 결과 표시</p>

<ul><li>마운트 위치 테이블 정보 수신.</li><li>FKP, VRS mount 기준국 접속 가능.</li><li>접속계정 데이타 DB로 관리</li><li>GPS NMEA 데이타 로거로도 적용 (위성지도 연동) </li></ul>

<h2 id="designing-tcp-network-codes">Designing TCP network codes</h2>
<center><img src="./assets/img/posts/20210318/tcp_block-code.png" width="540" /></center>
<p><br />
<small>[Block diagram] TCP fgv logic code</small></p>

<p>I started out with two hidden layers of 36 neurons each.</p>

<h2 id="source-code--github">Source code / Github</h2>
<h3 id="python---labview">Python -&gt; LabVIEW</h3>

<p>참고 python 코드 깃헙 <a href="https://github.com/tridge/pyUblox/blob/master/ntrip.py">tridge/pyUblox</a> Ntrip client 소스코드.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">header</span> <span class="o">=</span>\
<span class="s">"GET /{} HTTP/1.1</span><span class="se">\r\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mountpoint</span><span class="p">)</span> <span class="o">+</span>\
<span class="s">"Host </span><span class="se">\r\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">server</span><span class="p">)</span> <span class="o">+</span>\
<span class="s">"Ntrip-Version: Ntrip/2.0</span><span class="se">\r\n</span><span class="s">"</span> <span class="o">+</span>\
<span class="s">"User-Agent: NTRIP pyUblox/0.0</span><span class="se">\r\n</span><span class="s">"</span> <span class="o">+</span>\
<span class="s">"Connection: close</span><span class="se">\r\n</span><span class="s">"</span> <span class="o">+</span>\
<span class="s">"Authorization: Basic {}</span><span class="se">\r\n\r\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pwd</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">cycle</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span><span class="o">+</span><span class="p">(</span><span class="n">max_lr</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p><br />With these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves6.png" width="540" />
<small>1,000,000 episodes, 7.5 million epochs with batches of 64 moves each<br />
Wins: 52.66% Losses: 36.02% Ties: 11.32%</small></center>

<p>After <strong>24 hours!</strong>, my computer 
<a name="Model3"></a></p>
<h3 id="model-3---new-network-topology">Model 3 - new network topology</h3>

<p>After all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.</p>

<p><strong>Finally tested FKP,VRS</strong> This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.</p>

<p>It is quite interesting :</p>
<ul>
  <li>the rewards policy</li>
  <li>the epsilon greedy strategy</li>
  <li>whether to train vs. a random player or an “intelligent” AI.</li>
</ul>

<p>And so far the most effective change</p>

<p><a name="Model4"></a></p>
<h3 id="model-4---implementing-momentum">Model 4 - implementing momentum</h3>

<p>I <a href="https://www.reddit.com/r/MachineLearning/comments/lzvrwp/p_help_with_a_reinforcement_learning_project/">reached out to the reddit community</a> and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm.</p>

<ul>
  <li>Stochastic Gradient Descent with Momentum</li>
  <li>RMSProp: Root Mean Square Plain Momentum</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span>
                           <span class="n">units</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span>
                           <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                           <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span>
                        <span class="n">outputs</span><span class="p">,</span>
                        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                        <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
           <span class="n">beta_1</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">GAMMA_OPT</span><span class="p">,</span>
           <span class="n">beta_2</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BETA</span><span class="p">,</span>
           <span class="n">epsilon</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">,</span>
           <span class="n">amsgrad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
                           <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                           <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>
<p>As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.</p>

<p>With Tensorflow implemented, <strong>the loss function was still stagnating! My code was not the issue.</strong>
<a name="Model7"></a></p>
<h3 id="model-7---changing-the-training-schedule">Model 7 - changing the training schedule</h3>
<p>Next I tried to change the way the network was training as per <a href="https://www.reddit.com/user/elBarto015">u/elBarto015</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lzzjar/i_created_an_ai_for_super_hexagon_based_on/gqc8ka6?utm_source=share&amp;utm_medium=web2x&amp;context=3">advised me on reddit</a>.</p>

<p>The way I was training initially was:</p>
<ul>
  <li>Games begin being simulated and the outcome recorded in the replay memory</li>
  <li>Once a sufficient ammount of experiences are recorded (at least equal to the batch</li>
</ul>

<p><img src="./assets/img/posts/20210318/statistics.jpg" alt="tcp_block" />
<small>[tcp_block] LabVIEW TCP Function Block Diagram code.</small></p>

<p>As of today, <a href="https://github.com/yunbum/NtripClient">I converting RC Connect to Linux / Ubuntu version</a>, and it looks like a viable option to test and a fun coding challenge.</p>
:ET