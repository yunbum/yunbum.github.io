I"¤,<p>SRC ì€ GPS, Camera, Lidar, IMU, ë“±ì˜ ì„¼ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ììœ¨ì£¼í–‰ êµìœ¡ ë° ì£¼í–‰ì•Œê³ ë¦¬ì¦˜, ì„¼ì„œí“¨ì „ ë“±ì„ ì‰½ê³  íš¨ê³¼ì ìœ¼ë¡œ ê°œë°œ í•  ìˆ˜ ìˆëŠ” ììœ¨ì£¼í–‰ ëª¨ë°”ì¼ë¡œë´‡ í”Œë«í¼ ì…ë‹ˆë‹¤.:</p>
<ul>
  <li><a href="https://github.com/yunbum/SRC">This Powerful Self Driving HW Led to Developer and students in this field.</a></li>
  <li><a href="https://cafe.naver.com/iltech">Check detail picture, code examples and hw information</a></li>
  <li><a href="https://www.youtube.com/channel/UCd23NgICe3702uqAAk4HYFQ/videos">Other videos on Youtube for SRC</a></li>
</ul>

<p><img src="./assets/img/posts/20210402/src_hw-sw.png" alt="HW SW ëª¨ë“ˆê°œë°œ ë° ì£¼í–‰í…ŒìŠ¤íŠ¸" />
<small>ë‹¤ì–‘í•œ HW ë° SW ëª¨ë“ˆë¡œ ì£¼í–‰ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ ë° ì„¼ì„œí“¨ì „ í…ŒìŠ¤íŠ¸ì— ì ìš©ì™„ë£Œ </small></p>

<p>ì°¨ëŸ‰ì œì–´ëŠ” Python, C, ë“±ì˜ ì»´í“¨í„°ì–¸ì–´ì™€ë„ í˜¸í™˜ì´ ë˜ë„ë¡ ì‹œë¦¬ì–¼í†µì‹ ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LabVIEW ë¼ê³ í•˜ëŠ” National Instrument ì‚¬ì˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë„ ì§€ì›í•©ë‹ˆë‹¤.</p>

<p>SRC (<strong>S</strong>elf driving <strong>R</strong>emote control <strong>C</strong>ar) ëŠ” ììœ¨ì£¼í–‰ ì°¨ëŸ‰ì˜ ì˜ì–´ ì•½ì ì´ë©´ í˜„ì¬ SRC-A,B,C,D ë“±ì˜ íƒ€ì…ì´ ìˆìŠµë‹ˆë‹¤.
<img src="./assets/img/posts/20210402/SRC_models.png" alt="SRC models" />
<small>ê°œë°œì‹œê°„ ì „ì²´ ì•½ 10ì—¬ë…„. ìµœì¢… ì „ì²´ ê¸ˆì†ê¸°ë°˜ì˜ êµ¬ì¡°ëŠ” 2019ë…„ ì™„ì„±</small></p>

<p>ì£¼ìš”íŠ¹ì§•: í”„ë ˆì„ì€ ì „ì²´ê°€ ì•Œë£¨ë¯¸ëŠ„, ì²  ë“±ì˜ ê¸ˆì†. ì¼ë°˜ íœ´ëŒ€ìš© ë³´ì¡°ë°°í„°ë¦¬ íŒŒì›Œ. í†µ ê³ ë¬´ íƒ€ì´ì–´.  (<a href="https://www.thinkautomation.com/bots-and-ai/a-history-of-automation-the-rise-of-robots-and-ai/">source</a>), êµ¬ì¡°ì™€ ê°•ì„±ì´ ìƒë‹¹íˆ ê°œëŸ‰ë˜ì–´ ì „ì²´ì ì¸ ì•ˆì •ì„±ì€ ì¼ë°˜ í”Œë¼ìŠ¤í‹± RC ì¹´ì— ë¹„í•  ìˆ˜ ì—†ê³ , í˜„ì¬ ìˆ˜ì‹­ km ì‹¤ì™¸ ìš´í–‰ìœ¼ë¡œ ë‚´êµ¬ì„± ê²€ì¦ë„ ì™„ë£Œ:</p>

<p><img src="./assets/img/posts/20210402/src-b2_3.jpg" alt="Frame body" />
<small>ê°œë°œì‹œê°„ ì „ì²´ ì•½ 10ì—¬ë…„. ìµœì¢… ì „ì²´ ê¸ˆì†ê¸°ë°˜ì˜ êµ¬ì¡°ëŠ” 2019ë…„ ì™„ì„±</small></p>

<p>ê°œë³„ ëª¨ë“ˆë“¤ì˜ êµ¬ì„±ì€ ë§ì¶¤í˜•ìœ¼ë¡œ ì œê³µ ë  ìˆ˜ ìˆìœ¼ë©°, ê¸°ë³¸ ëª¨í„°ì™€ ëª¨í„°ë“œë¼ì´ë²„, ìƒìœ„ì œì–´ê¸°(ì•„ë‘ì´ë…¸) ë§Œìœ¼ë¡œë„ ì œê³µì´ ë˜ë©° ì œì–´ëª…ë ¹ì€ USB ì„ í†µí•œ ì‹œë¦¬ì–¼ í†µì‹ ìœ¼ë¡œ ì œì–´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤..</p>

<p><img src="./assets/img/posts/20210402/SRC-B_parts.png" alt="sensors &amp; modules" />
<small>Full option ìƒíƒœì˜ ì„¼ì„œ ë° ê¸°íƒ€ ëª¨ë“ˆ êµ¬ì„±ë„</small></p>

<p>SRC model / SRC ì°¨ëŸ‰ì˜ ëª¨ë¸ì€ A,B,C,D íƒ€ì…ë“±ìœ¼ë¡œ ëª¨í„°ì‚¬ì–‘, í¬ê¸°, ë””ìì¸ ë“±ìœ¼ë¡œ êµ¬ë¶„ì´ ë˜ì–´ ë‚˜ë‰˜ê³  ë™ì¼í•œ ëª¨ë¸ì—ì„œëŠ” ì´ì¤‘ëª¨í„°ë¡œ ì—…ê·¸ë ˆì´ë“œê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. (Dual moter) .</p>

<p><img src="./assets/img/posts/20210402/SRC_models.png" alt="A perceptron" />
<small>Full option ìƒíƒœì˜ ì„¼ì„œ ë° ê¸°íƒ€ ëª¨ë“ˆ êµ¬ì„±ë„</small></p>

<p>We can imagine then that if many of the eye cells of the fly produce 1s, it means that an object is quite near, and therefore the perceptron will calculate a 1, it is time to flee.</p>

<p><img src="./assets/img/posts/20210402/post7-fly-vision.jpg" alt="The fly vision" /></p>

<p>The perceptron is just a math operation, one that multiplies certain input values with preset â€œparametersâ€ (called weights) and adds up the resulting multiplications to generate a value.</p>

<p>Then the magic spark was ignited, the parameters (weights) of the perceptron could be â€œlearntâ€ by a process of minimizing the difference between known results of particular observations, and what the perceptron is actually calculating. It is this process of learning what we call <strong>training the neural network</strong>.</p>

<tweet>This idea is so powerful that even today it is one of the fundamental building blocks of what we call AI.</tweet>

<p>From this I will try to explain how this simple concept can have such diverse applications as natural language processing (think Alexa), image recognition like medical diagnosis from a CTR scan, autonomous vehicles, etc.</p>

<p>A basic neural network is a combination of perceptrons in different arrangements, the perceptron therefore was downgraded from â€œfly brainâ€ to â€œnetwork neuronâ€.
<img src="./assets/img/posts/20210402/post7-multilayer-perceptron.png" alt="A multilayer perceptron" /></p>

<p>A neural network has different components, in its basic form it has:</p>
<ul>
  <li>Input</li>
  <li>Hidden layers</li>
  <li>Output</li>
</ul>

<p><img src="./assets/img/posts/20210228/nnet_flow.gif" alt="Neural network components" /></p>

<h3 id="input">Input</h3>

<p>The inputs of a neural network are in their essence just numbers, therefore anything that can be converted to a number can become an input. Letters in a text, pixels in an image, frequencies in a sound wave, values from a sensor, etc. are all different things that when converted to a numerical value serve as inputs for the neural network. This is one of the reasons why applications of neural networks are so diverse.</p>

<p>Inputs can be as many as one need for the task at hand, from maybe 9 inputs to teach a neural network how to play tic-tac-toe to thousands of pixels from a camera for an autonomous vehicle. Since the input of a perceptron needs to be a single value, if for example a color pixel is chosen as input, it most likely will be broken into three different values; its  red, green and blue components, hence each pixel will become 3 different inputs for the neural network.</p>

<h3 id="hidden-layers">Hidden layers</h3>

<p>A â€œlayerâ€ within a neural network is just a group of perceptrons that all perform the same exact mathematical operation to the inputs and produce an output. The catch is that each of them have different weights (parameters), therefore their output for a given input will be different amongst them. There are many types of layers, the most typical of them being a â€œdenseâ€ layer, which is another word to say that all the inputs are connected to all the neurons (individual perceptrons), and as said before, each of these connections have a weight associated with it, so that the operation that each neuron performs is a simple weighted sum of all the inputs.</p>

<p><img src="./assets/img/posts/20210402/post7-dense-layers.png" alt="post7-dense-layers" /></p>

<p>The hidden layer is then typically connected to another dense layer, and their connection means that each output of a neuron from the first layer is treated effectively as an input for the subsequent one, and it is thus connected to every neuron.</p>

<p>A neural network can have from one to as many layers as one can think, and the number of layers depends solely on the experience we have gathered on the particular problem we would like to solve.</p>

<p>Another critical parameter of a hidden layer is the number of neurons it has, and again, we need to rely on experience to determine how many neurons are needed for a given problem. I have seen networks that vary from a couple of neurons to the thousands. And of course each hidden layer can have as many neurons as we please, so the number of combinations is vast.</p>

<p>To the number of layers, their type and how many neurons each have, is what we call the <em>network topology</em> (including the number of inputs and outputs).</p>

<h3 id="output">Output</h3>

<p>At the very end of the chain, another layer lies (which behaves just like a hidden layer), but has the peculiarity that it is the final layer, and therefore whatever it calculates will be the output values of the whole network. The number of outputs the network has is a function of the problem we would like to solve. It could be as simple as one output, with its value representing a probability of an action (like in the case of the flee reaction of the housefly), to many outputs, perhaps if our network is trying to distinguish images of animals, one would have an output for each animal species, and the output would represent how much confidence the network has that the particular image belongs to the corresponding species.</p>

<p><img src="./assets/img/posts/20210402/post7-alexa.png" alt="Alexa recognizing speach" /></p>

<p>Things like â€œAlexaâ€, are a bit more complex, but work on exactly the same principles. Letâ€™s break down for example the case of asking â€œAlexaâ€ to play a song in spotify. Alexa uses several different neural networks to acomplish this:</p>

<h4 id="1-speech-recognition">1. Speech recognition</h4>

<p>As a basic input we have our speech: the command <strong>â€œAlexa, play Van Halenâ€</strong>. This might seem quite simple for us humans to process, but for a machine is an incredible difficult feat to be able to understand speech, things like each individual voice timbre, entonation, intention and many more nuances of human spoken language make it so that traditional algorithms have struggled a lot with this. In our simplified example letâ€™s say that we use a neural network to transform our spoken speech into text characters a computer is much more familiarized to learn.</p>

<h4 id="2-understanding-what-we-mean-natural-language-understanding">2. Understanding what we mean (Natural Language Understanding)</h4>

<p>Once the previous network managed to succesfuly convert our spoken words into text, there comes the even more difficult task of making sense of what we said. Things that we humans take for granted such as context, intonation and non verbal communication, help give our words meaning in a very subtle, but powerful way, a machine will have to do with much less information to correctly understand what we mean. It has to correctly identify the intention of our sentence and the subject or entities of what we mean.</p>

<p><img src="./assets/img/posts/20210402/post7-alexa-natural-lang.png" alt="post7-alexa-natural-lang" /></p>

<p>The neural network has to identify that it received a command (by identifying its name), the command (â€œplay musicâ€), and our choice (â€œVan Halenâ€). And it does so by means of simple math operations as described before. Of course the network involved is quite complex and has different types of neurons and connection types, but the underlying principles remain.</p>

<h4 id="3-replying-to-us">3. Replying to us</h4>

<p>Once Alexa understood what we meant, it then proceeds to execute the action of the command it interpreted and it replies to us in turn using natural language. This is accomplished using a technique called speech synthesis, things like pitch, duration and intensity of the words and phonems are selected based on the â€œmeaningâ€ of what Alexa will respond to us: â€œPlaying songs by Van Halen on Spotifyâ€ sounding quite naturally. And all is accomplished with neural networks executing many simple math operations.</p>

<p><img src="./assets/img/posts/20210402/post7-alexa-steps.png" alt="post7-alexa-steps" />
<small>Although it seems quite complex, the process for AI to understand us can be boiled down to simple math operations</small></p>

<p>Of course Amazonâ€™s Alexa neural networks have undergone quite a lot of training to get to the level where they are, the beauty is that once trained, to perform their magic they just need a few mathematical operations.</p>

<p>As said before, I will continue to write about the basics of neural networks, the next article in the series will dive a bit deeper into the math behind a basic neural network.</p>

:ET