I"ı,<h2 id="background">Background</h2>
<p>GPS ì˜ ì •ë°€ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ RTK mode ì¤‘ Network RTK ê¸°ëŠ¥ ì„¤ì •ì„ ìœ„í•´ í•„ìš”í•œ Ntrip client íˆ´ì„ ì§ì ‘ ê°œë°œí•˜ì—¬ ì‚¬ìš©í•˜ê³ , ë³„ë¡œë„ ë…ë¦½ íˆ´ë¡œ ê°œë°œí•˜ì—¬ ë°°í¬í•©ë‹ˆë‹¤..</p>

<p><img src="./assets/img/posts/20210318/RL_Connect.jpg" alt="RL_Connect" />
<small>[RL_Connect] Netwrok RTK ëª¨ë“œ ì„¤ì •ì„ ìœ„í•œ Ntrip client..</small></p>

<center><img style="float: left;margin-right: 1em;" src="./assets/img/posts/20210318/tm_circle.png" width="310" height="300" /></center>

<p>Ntrip client ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ ë³¸ í•˜ë“œì›¨ì–´ë¡œëŠ” uBlox(F9P, M8P), Septentrio(Mosaic X5) ë“±ì˜ ì œí’ˆì´ ìˆìŠµë‹ˆë‹¤.</p>

<ul><li>ë§ˆìš´íŠ¸ ìœ„ì¹˜ í…Œì´ë¸” ì •ë³´ ìˆ˜ì‹ .</li><li>FKP, VRS mount ê¸°ì¤€êµ­ ì ‘ì† ê°€ëŠ¥.</li><li>ì ‘ì†ê³„ì • ë°ì´íƒ€ DBë¡œ ê´€ë¦¬</li><li>GPS NMEA ë°ì´íƒ€ ë¡œê±°ë¡œë„ ì ìš© (ìœ„ì„±ì§€ë„ ì—°ë™) </li></ul>

<h2 id="designing-tcp-network-codes">Designing TCP network codes</h2>

<center><img src="./assets/img/posts/20210318/tcp_block-code.png" width="540" /></center>
<p><br />
<small>[Block diagram] TCP fgv logic code</small></p>

<p>I started out with two hidden layers of 36 neurons each.</p>

<h2 id="the-many-trial-errors">The many trial errors</h2>
<h3 id="ì‹œí–‰ì°©ì˜¤-1---the-first-try">ì‹œí–‰ì°©ì˜¤ 1 - the first try</h3>

<p>At first the model was trained by playing vs. a â€œperfectâ€ AI, meaning a <a href="https://github.com/amaynez/TicTacToe/blob/b429e5637fe5f61e997f04c01422ad0342565640/entities/Game.py#L43">hard coded algorithm</a> that</p>

<p>ì°¸ê³  python ì½”ë“œ ê¹ƒí—™ <a href="https://github.com/tridge/pyUblox/blob/master/ntrip.py">tridge/pyUblox</a> Ntrip client ì†ŒìŠ¤ì½”ë“œ.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">header</span> <span class="o">=</span>\
<span class="s">"GET /{} HTTP/1.1</span><span class="se">\r\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mountpoint</span><span class="p">)</span> <span class="o">+</span>\
<span class="s">"Host </span><span class="se">\r\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">server</span><span class="p">)</span> <span class="o">+</span>\
<span class="s">"Ntrip-Version: Ntrip/2.0</span><span class="se">\r\n</span><span class="s">"</span> <span class="o">+</span>\
<span class="s">"User-Agent: NTRIP pyUblox/0.0</span><span class="se">\r\n</span><span class="s">"</span> <span class="o">+</span>\
<span class="s">"Connection: close</span><span class="se">\r\n</span><span class="s">"</span> <span class="o">+</span>\
<span class="s">"Authorization: Basic {}</span><span class="se">\r\n\r\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pwd</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">staticmethod</span>
<span class="k">def</span> <span class="nf">cyclic_learning_rate</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">max_lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">MAX_LR_FACTOR</span>
    <span class="n">cycle</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="p">(</span><span class="n">epoch</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="n">c</span><span class="p">.</span><span class="n">LR_STEP_SIZE</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">cycle</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">learning_rate</span><span class="o">+</span><span class="p">(</span><span class="n">max_lr</span><span class="o">-</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p><br />With these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.</p>

<center><img src="./assets/img/posts/20210318/Loss_function_and_Illegal_moves6.png" width="540" />
<small>1,000,000 episodes, 7.5 million epochs with batches of 64 moves each<br />
Wins: 52.66% Losses: 36.02% Ties: 11.32%</small></center>

<p>After <strong>24 hours!</strong>, my computer 
<a name="Model3"></a></p>
<h3 id="model-3---new-network-topology">Model 3 - new network topology</h3>

<p>After all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.</p>

<p><strong>Finally tested FKP,VRS</strong> This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.</p>

<p>It is quite interesting :</p>
<ul>
  <li>the rewards policy</li>
  <li>the epsilon greedy strategy</li>
  <li>whether to train vs. a random player or an â€œintelligentâ€ AI.</li>
</ul>

<p>And so far the most effective change</p>

<p><a name="Model4"></a></p>
<h3 id="model-4---implementing-momentum">Model 4 - implementing momentum</h3>

<p>I <a href="https://www.reddit.com/r/MachineLearning/comments/lzvrwp/p_help_with_a_reinforcement_learning_project/">reached out to the reddit community</a> and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm.</p>

<ul>
  <li>Stochastic Gradient Descent with Momentum</li>
  <li>RMSProp: Root Mean Square Plain Momentum</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">hidden_layers</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span>
                           <span class="n">units</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span>
                           <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
                           <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                           <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span>
                        <span class="n">outputs</span><span class="p">,</span>
                        <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                        <span class="n">bias_initializer</span><span class="o">=</span><span class="s">'zeros'</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">LEARNING_RATE</span><span class="p">,</span>
           <span class="n">beta_1</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">GAMMA_OPT</span><span class="p">,</span>
           <span class="n">beta_2</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">BETA</span><span class="p">,</span>
           <span class="n">epsilon</span><span class="o">=</span><span class="n">c</span><span class="p">.</span><span class="n">EPSILON</span><span class="p">,</span>
           <span class="n">amsgrad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">PolicyNetwork</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
                           <span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span>
                           <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>
<p>As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.</p>

<p>With Tensorflow implemented, <strong>the loss function was still stagnating! My code was not the issue.</strong>
<a name="Model7"></a></p>
<h3 id="model-7---changing-the-training-schedule">Model 7 - changing the training schedule</h3>
<p>Next I tried to change the way the network was training as per <a href="https://www.reddit.com/user/elBarto015">u/elBarto015</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lzzjar/i_created_an_ai_for_super_hexagon_based_on/gqc8ka6?utm_source=share&amp;utm_medium=web2x&amp;context=3">advised me on reddit</a>.</p>

<p>The way I was training initially was:</p>
<ul>
  <li>Games begin being simulated and the outcome recorded in the replay memory</li>
  <li>Once a sufficient ammount of experiences are recorded (at least equal to the batch</li>
</ul>

<center><img src="./assets/img/posts/20210318/ReplayMemoryBefore.png" width="540" /></center>

<p>As of today, <a href="https://medium.com/applied-data-science/how-to-train-ai-agents-to-play-multiplayer-games-using-self-play-deep-reinforcement-learning-247d0b440717">self play</a>, and it looks like a viable option to test and a fun coding challenge.</p>
:ET